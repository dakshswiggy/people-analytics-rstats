---
title: "Statistical Inference and Linear Regression"
author: "Keith McNulty"
output:
  xaringan::moon_reader:
    css: "style.css"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

class: left, middle, rstudio-logo, bigfont

## Aims of this module

&#9989; Understand the concept of statistical inference
  - Briefly review some important statistical concepts
  - Review an example of a statistical hypothesis test

&#9989; Learn our first explanatory modeling method
  - Review simple and multiple Linear Regression
  - Learn some foundational concepts that apply to other model types

---
class: left, middle, rstudio-logo

# Statistical Inference

---
class: left, middle, rstudio-logo

## Samples and Populations

In the vast majority of situations when we apply statistics to a problem, we are being asked to draw a conclusion about a *population*, but we only have data on a *sample* subset of that population.

What might be the sample and what might be the population in each of these problems?

1.  A political election forecast
2.  A market research survey for a grocery chain
3.  An employer trying to understand if compensation levels may be a factor in employee retention

No matter what we see in a sample, we can never be 100% certain that we would see the same in the population, but sometimes we can be certain enough to *infer* that we will see the same in the population.  The mathematics behind this process is known as *statistical inference*. 

---
class: left, middle, rstudio-logo

## Let's look at an example

Let's take the charity donation dataset that you worked on in the exercises in the last module, and let's determine the mean age of ten randomly selected people who made donations.  Note that the age of a person is considered a *random variable*, in that each persons age is independently drawn from the same overall distribution.

```{r}
url <- "https://peopleanalytics-regression-book.org/data/charity_donation.csv"
charity_data <- read.csv(url)

# select ages of ten people at random
set.seed(123) # <- ensures we can reproduce
random_ages <- sample(charity_data$age, 100)

#take mean
(mean_age <- mean(random_ages))

```

So can we conclude that people who donate to this charity have a mean age of `r mean_age`?


---
class: left, middle, rstudio-logo

## Repeated sampling tells us about a pattern we can expect

If we were to do this process 1000 times, and draw a density plot, this is what it would look like.  You can see the code for this in the source of this document.

```{r, echo = FALSE, out.height = "350", out.extra='style="float:left; padding:10px"'}
library(ggplot2)

sample_means <- c()

set.seed(123)
for (i in 1:1000) {
  sample_means[i] <- mean(sample(charity_data$age, 100))
}


ggplot() +
  geom_density(aes(x = sample_means), fill = "pink", alpha = 0.3) +
  geom_vline(xintercept = mean(sample_means), color = "blue") +
  geom_vline(xintercept = mean(sample_means) + sd(sample_means), color = "red", linetype = "dashed") +
  geom_vline(xintercept = mean(sample_means) - sd(sample_means), color = "red", linetype = "dashed") +
  geom_vline(xintercept = mean(sample_means) - 1.96*sd(sample_means), color = "brown", linetype = "dashed") +
  geom_vline(xintercept = mean(sample_means) + 1.96*sd(sample_means), color = "brown", linetype = "dashed") +
  labs(x = "Sample Mean", y = "Density") +
  annotate(geom = "text", x = 46.7, y = 0.29, label = "Mean", color = "blue") +
  annotate(geom = "text", x = 45.4, y = 0.29, label = "-1 SE", color = "red") +
  annotate(geom = "text", x = 44.0, y = 0.29, label = "-1.96 SE", color = "brown") +
  annotate(geom = "text", x = 48.7, y = 0.29, label = "+1 SE", color = "red") +
  annotate(geom = "text", x = 50.1, y = 0.29, label = "+1.96 SE", color = "brown") +
  theme_minimal()
```

Our density plot shows the following properties:

* Has the shape of a normal (Gaussian) distribution
* Blue line as the mean value (the mean of the sample means) - most likely value for the population mean
* Red dashed lines as +/- 1 standard deviations - 69% probability that population mean is in this range
* Brown dashed lines as +/- 1.96 standard deviations - 95% probability that population mean is in this range

---
class: left, middle, rstudio-logo

## This observation leads to some important concepts

1.  The expected mean over many samples of a random variable is itself a random variable, with a mean value and a distribution around that mean.
2.  Given this distribution, we call a standard deviation above or below the expected mean of a random variable a *standard error (SE)* for the population mean
3.  We call a probability range around the expected mean of a random variable a *confidence interval* for the population mean.  This confidence interval corresponds to specified multiples of standard errors.

As a consequence of this, if we know the mean of our sample, we can use the expected distribution of that mean to determine the standard error and the confidence interval.  

Often as a 'rule of thumb', practitioners will regard +/- 2 standard errors as the 95% confidence interval.


---
class: left, middle, rstudio-logo

## Hypothesis testing

Now imagine someone asks us a question about differences between populations.  For example:  do men and women that donate to our charity have a different average age?

We do not have the data for all men and women that donate, but we do have the data from our sample dataset. Let's calculate the mean age of men and of women in that data set.

```{r}
age_m <- charity_data$age[charity_data$gender == "M"]
mean(age_m)
```

```{r}
age_f <- charity_data$age[charity_data$gender == "F"]
mean(age_f)
```

So in our sample there is an age difference of about 0.9 years on average.  We now need to determine if that age difference is large enough to infer that there is a difference in the average age of all men who donate vs all women who donate.


---
class: left, middle, rstudio-logo

## Process of statistically testing a hypothesis of difference

First we note that the difference of two random variables is a random variable, so in this case we can expect the difference between the mean age of men and the mean age of women to obey an expected sampling distribution with a standard error and a 95% confidence interval for the population value.

So we can use the following process to test our whether we can infer that there is a difference:

* Assume as a starting point that the difference in the population is actually zero.  This is called the *null hypothesis*. 
* Use the sample to calculate the expected mean, standard error and 95% confidence interval for the difference between the mean ages in the population.
* Determine whether or not zero lies inside or outside the 95% confidence interval.  If it lies outside, this means that there is less than a 5% chance that this sample would occur if the null hypothesis were true.
* If there is less than a 5% chance that this sample would occur if the null hypothesis were true, we infer that the null hypothesis should be rejected and we conclude the *alternative hypothesis*.
* Otherwise we do not infer that the null hypothesis can be rejected.





---
class: left, middle, rstudio-logo

## Welch's $t$-test of difference in means

Welch's $t$-test will perform all of these steps for you.  It will calculate the expected distribution of the difference in means based on your samples, it will compute the 95% confidence interval, and it will tell you where zero lies on the distribution.  In this case it tells us that we cannot infer a rejection of the null hypothesis.

```{r}
t.test(age_m, age_f)
```

---
class: left, middle, rstudio-logo

## $\alpha$ and the $p$-value

The $p$-value returned by a hypothesis test represents the likelihood of seeing this sample difference or a larger difference if the null hypothesis were true.   

```{r, echo = FALSE, out.height = "350", out.extra='style="float:left; padding:10px"'}
diff <- mean(age_m) - mean(age_f)
test <- t.test(age_m, age_f)
lower <- test$conf.int[1]
upper <- test$conf.int[2]
tval <- test$statistic
df <- test$parameter
se <- test$stderr
breaks <- -2:2
labels <- round(diff + se*breaks, 2)

ggplot(data.frame(x = c(-2.5, 2.5)), aes(x = x)) +
  stat_function(fun = dt, args = list(df = df), color = "blue") +
  scale_x_continuous(breaks = breaks,
                     labels = labels) +
  geom_vline(xintercept = tval, linetype = "dashed", color = "red") +
  geom_vline(xintercept = -tval, linetype = "dashed", color = "red") +
  annotate(geom = "text", x = 1.2, y = 0.4, 
           label = "Mean M - Mean F = 0", color = "red") +
  annotate(geom = "text", x = -1.2, y = 0.4, 
           label = "Mean F - Mean M = 0", color = "red") +
  annotate(geom = "text", x = -1, y = 0.1, label = "A", size = 10) +
  annotate(geom = "text", x = 0, y = 0.1, label = "B", size = 10) +
  annotate(geom = "text", x = 1, y = 0.1, label = "C", size = 10) +
  labs(x = "Estimated population diff",
       y = "Density") +
  theme_minimal()
```

* The red dashed lines represent the points at which the difference is zero (first line is F - M, second line is M - F)
* This splits the density curve into 3 segments.
* The total area of A + C represents the $p$-value - it is the likelihood that samples with this difference or a larger difference would occur assuming there was a zero difference in the population.
* If this $p$-value is less than a specified standard known as $\alpha$, we can infer a rejection of the null hypothesis.  Usually $\alpha = 0.05$.  

---
class: left, middle, rstudio-logo

## Exercise - Hypothesis testing for a difference in means

For our next short exercise, we will do some practice on running a hypothesis test for a difference in means.

Go to our [RStudio Cloud workspace](https://rstudio.cloud/spaces/230780/join?access_code=7cXJKFU1KUuuZGLwBVQpLG3dIxPUD3jak3ZQmESh) and start **Assignment 02A - Statistical Inference**.

Let's work on **Exercises 1 and 2**.


---
class: left, middle, rstudio-logo

## Using regression models for statistical inference

Using similar principles to the simple hypothesis test we have just seen, we use regression modeling techniques on samples in order to make inferences about populations.  In most modeling scenarios we will have the following at the outset:

1.  A sample $S$ of observations taken from a population $P$
2.  A dataset containing data for our sample set $S$.  The data includes a set of input variables $x_1, x_2, ..., x_p$ and an outcome variable $y$.
3.  A model type to relate $x_1, x_2, ..., x_p$ to $y$.  We select this model type for the most part based on the scale of our outcome data $y$.

We will use our sample dataset to estimate the parameters for the model type we have selected, and we will use these parameters to make inferences about the following:

1.  Which input variables influence the outcome for the population?
2.  To what degree does each input variable influence the outcome?
3.  How much of the outcome is explained by all our input variables?

---
class: left, middle, rstudio-logo

## Choosing your model type

Regression model types are usually chosen based on the scale of the outcome variable $y$.  

```{r, echo = FALSE}
library(kableExtra)

d <- data.frame(
  Outcome = c(
    "Continuous scale (eg money, height, weight)",
    "Binary scale (Yes/No)",
    "Nominal category scale (eg A, B, C)",
    "Ordinal category scale (eg Low, Medium, High)",
    "Time dependent binary scale"
  ),
Model = c(
  "Linear regression",
  "Binary logistic regression",
  "Multinomial logistic regression",
  "Ordinal logistic regression",
  "Survival/proportional hazard regression"
  )
)

kbl(d) |> 
  kable_minimal() |> 
  row_spec(1, background = "lightblue")
```


---
class: left, middle, rstudio-logo

# Linear regression modeling
